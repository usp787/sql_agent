{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen 2.5 Coder 7B Instruct - Local Testing Notebook\n",
    "\n",
    "## Hardware Requirements\n",
    "\n",
    "### Minimum Requirements:\n",
    "- **RAM**: 16 GB system RAM (for model loading and inference)\n",
    "- **VRAM**: 8 GB GPU memory (for FP16/BF16)\n",
    "- **Storage**: ~15 GB free disk space (for model weights)\n",
    "- **CPU**: Modern multi-core processor (if running CPU-only)\n",
    "\n",
    "### Recommended Requirements:\n",
    "- **RAM**: 32 GB system RAM\n",
    "- **VRAM**: 16 GB GPU memory (NVIDIA RTX 3090, 4090, A4000, etc.)\n",
    "- **Storage**: 20 GB SSD\n",
    "- **GPU**: CUDA-compatible GPU (compute capability 7.0+)\n",
    "\n",
    "### Quantization Options (to reduce memory usage):\n",
    "- **8-bit quantization**: ~4 GB VRAM required\n",
    "- **4-bit quantization**: ~2-3 GB VRAM required\n",
    "- **CPU-only mode**: 16+ GB RAM (slower inference)\n",
    "\n",
    "### Model Details:\n",
    "- **Model**: Qwen/Qwen2.5-Coder-7B-Instruct\n",
    "- **Parameters**: 7.61 billion\n",
    "- **Context Length**: 128K tokens\n",
    "- **Architecture**: Transformer-based decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "Run this cell first to install all dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -q torch>=2.0.0 transformers>=4.37.0 accelerate>=0.25.0 bitsandbytes>=0.41.0 sentencepiece>=0.1.99 protobuf>=3.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check System Information\n",
    "\n",
    "Let's verify your hardware setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM INFORMATION\n",
      "============================================================\n",
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.1\n",
      "GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU Memory: 8.00 GB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def check_system_info():\n",
    "    \"\"\"Display system information for debugging\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SYSTEM INFORMATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"Running on CPU (slower performance)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "check_system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Model Loading\n",
    "\n",
    "Choose your quantization option:\n",
    "- `None` - Full precision (FP16/BF16) - requires ~14 GB VRAM\n",
    "- `'8bit'` - 8-bit quantization - requires ~4 GB VRAM\n",
    "- `'4bit'` - 4-bit quantization - requires ~2-3 GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: Quantization = 4bit\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Change this based on your hardware\n",
    "USE_QUANTIZATION = '4bit'  # Options: None, '8bit', '4bit'\n",
    "\n",
    "print(f\"Configuration: Quantization = {USE_QUANTIZATION if USE_QUANTIZATION else 'None (Full Precision)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer\n",
    "\n",
    "This will download the model on first run (~14 GB). Subsequent runs will use the cached version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "Quantization: 4bit\n",
      "This may take a few minutes on first run...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading with 4-bit quantization (requires bitsandbytes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_model(use_quantization=None):\n",
    "    \"\"\"\n",
    "    Load Qwen 2.5 Coder 7B Instruct model\n",
    "    \n",
    "    Args:\n",
    "        use_quantization: None, '8bit', or '4bit'\n",
    "    \"\"\"\n",
    "    model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "    \n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(f\"Quantization: {use_quantization if use_quantization else 'None (FP16/BF16)'}\")\n",
    "    print(\"This may take a few minutes on first run...\")\n",
    "    print()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Configure model loading based on quantization\n",
    "    model_kwargs = {\n",
    "        \"trust_remote_code\": True,\n",
    "        \"device_map\": \"auto\"  # Automatically distribute across available devices\n",
    "    }\n",
    "    \n",
    "    if use_quantization == '8bit':\n",
    "        print(\"Loading with 8-bit quantization (requires bitsandbytes)\")\n",
    "        model_kwargs[\"load_in_8bit\"] = True\n",
    "    elif use_quantization == '4bit':\n",
    "        print(\"Loading with 4-bit quantization (requires bitsandbytes)\")\n",
    "        model_kwargs[\"load_in_4bit\"] = True\n",
    "    else:\n",
    "        # Use BF16 if available, otherwise FP16\n",
    "        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "            model_kwargs[\"torch_dtype\"] = torch.bfloat16\n",
    "            print(\"Using BF16 precision\")\n",
    "        else:\n",
    "            model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "            print(\"Using FP16 precision\")\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        **model_kwargs\n",
    "    )\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    print()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = load_model(use_quantization=USE_QUANTIZATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=512, temperature=0.7):\n",
    "    \"\"\"Generate a response from the model\"\"\"\n",
    "    \n",
    "    # Format the prompt using chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to same device as model\n",
    "    if torch.cuda.is_available():\n",
    "        model_inputs = model_inputs.to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    print(\"Generating response...\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated tokens (exclude the prompt)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] \n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"Generation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Model\n",
    "\n",
    "Let's test with some example prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: Write a Python function to calculate the fibonacci sequence recursively.\n",
      "============================================================\n",
      "Generating response...\n",
      "\n",
      "Response:\n",
      "Sure! Here's an example implementation of a recursive function in Python that calculates the Fibonacci sequence:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return \"Input should be a positive integer.\"\n",
      "    elif n == 1:\n",
      "        return 0\n",
      "    elif n == 2:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "```\n",
      "\n",
      "This function takes an input `n` and returns the nth number in the Fibonacci sequence. It uses recursion to calculate the previous two numbers in the sequence (i.e., `fibonacci(n-1)` and `fibonacci(n-2)`) until it reaches the base cases where `n` is either 1 or 2.\n",
      "\n",
      "Here's how you can use this function:\n",
      "\n",
      "```python\n",
      "print(fibonacci(5))  # Output: 3\n",
      "print(fibonacci(7))  # Output: 8\n",
      "```\n",
      "\n",
      "Note that this implementation has exponential time complexity due to repeated calculations. For large values of `n`, it may take a long time to compute the result. If performance is a concern, consider using memoization or an iterative approach instead.\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Fibonacci sequence\n",
    "prompt1 = \"Write a Python function to calculate the fibonacci sequence recursively.\"\n",
    "# prompt1 = 'Write a python function to print hello world'\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: {prompt1}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "response1 = generate_response(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt1,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: Explain what is a binary search tree and implement it in Python.\n",
      "============================================================\n",
      "Generating response...\n",
      "\n",
      "Response:\n",
      "A Binary Search Tree (BST) is a type of binary tree where each node has at most two child nodes, referred to as the left child and the right child. The key property of a BST is that for any given node, all values in its left subtree are less than the value of the node, and all values in its right subtree are greater than the value of the node.\n",
      "\n",
      "Here's an implementation of a Binary Search Tree in Python:\n",
      "\n",
      "```python\n",
      "class Node:\n",
      "    def __init__(self, key):\n",
      "        self.left = None\n",
      "        self.right = None\n",
      "        self.val = key\n",
      "\n",
      "# Function to insert a new node with the given key\n",
      "def insert(root, key):\n",
      "    # If the tree is empty, return a new node\n",
      "    if root is None:\n",
      "        return Node(key)\n",
      "\n",
      "    # Otherwise, recur down the tree\n",
      "    if key < root.val:\n",
      "        root.left = insert(root.left, key)\n",
      "    else:\n",
      "        root.right = insert(root.right, key)\n",
      "\n",
      "    # return the (unchanged) node pointer\n",
      "    return root\n",
      "\n",
      "# A utility function to do inorder tree traversal\n",
      "def inorder_traversal(root):\n",
      "    if root:\n",
      "        inorder_traversal(root.left)\n",
      "        print(root.val)\n",
      "        inorder_traversal(root.right)\n",
      "\n",
      "# Driver program to test above functions\n",
      "root = None\n",
      "root = insert(root, 50)\n",
      "insert(root, 30)\n",
      "insert(root, 20)\n",
      "insert(root, 40)\n",
      "insert(root, 70)\n",
      "insert(root, 60)\n",
      "insert(root, 80)\n",
      "\n",
      "# Inorder traversal of the constructed BST will be\n",
      "print(\"Inorder traversal of the constructed BST:\")\n",
      "inorder_traversal(root)\n",
      "```\n",
      "\n",
      "This code defines a `Node` class for creating new nodes with a value and pointers to their left and right children. It includes a function `insert()` to add new keys to the BST while maintaining the BST properties. Lastly, there's a helper function `inorder_traversal()` which performs an in-order traversal of the BST, printing the values in ascending order.\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Binary Search Tree\n",
    "prompt2 = \"Explain what is a binary search tree and implement it in Python.\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: {prompt2}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "response2 = generate_response(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt2,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: Debug this code: def add(a b): return a + b\n",
      "============================================================\n",
      "Generating response...\n",
      "\n",
      "Response:\n",
      "The problem with the given code is that there is no space between 'a' and 'b' in the function definition. In Python, parameters should be separated by commas.\n",
      "\n",
      "Here's the corrected version of the code:\n",
      "\n",
      "```python\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "```\n",
      "\n",
      "Now you can call this function like `add(5, 3)` to get `8`.\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Debug code\n",
    "prompt3 = \"Debug this code: def add(a b): return a + b\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: {prompt3}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "response3 = generate_response(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt3,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Testing\n",
    "\n",
    "Use this cell to test your own prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your custom prompt here\n",
    "custom_prompt = \"Write a Python function to reverse a linked list.\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Your Prompt: {custom_prompt}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "custom_response = generate_response(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    custom_prompt,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(custom_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Configuration\n",
    "\n",
    "Adjust generation parameters for different behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_advanced(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt, \n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.0\n",
    "):\n",
    "    \"\"\"Generate response with advanced parameters\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model_inputs = model_inputs.to(model.device)\n",
    "    \n",
    "    print(f\"Generating with: temp={temperature}, top_p={top_p}, top_k={top_k}, rep_penalty={repetition_penalty}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] \n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "# Example with different temperature for more creative/deterministic output\n",
    "creative_prompt = \"Write a creative algorithm to solve the traveling salesman problem.\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing with higher temperature (more creative):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "creative_response = generate_response_advanced(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    creative_prompt,\n",
    "    temperature=0.9,  # Higher temperature = more creative\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(creative_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Memory Management\n",
    "\n",
    "If you need to free up memory, run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared\n",
      "Memory freed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "# Delete model and tokenizer\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear CUDA cache if using GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared\")\n",
    "\n",
    "print(\"Memory freed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### CUDA Out of Memory Error\n",
    "- Try 8-bit quantization: Set `USE_QUANTIZATION = '8bit'` and reload the model\n",
    "- Try 4-bit quantization: Set `USE_QUANTIZATION = '4bit'` and reload the model\n",
    "- Reduce `max_new_tokens` to generate shorter responses\n",
    "- Close other GPU-intensive applications\n",
    "\n",
    "### Model Download Issues\n",
    "- Ensure stable internet connection\n",
    "- The model (~14 GB) downloads automatically on first run\n",
    "- Cache location: `~/.cache/huggingface/hub/`\n",
    "\n",
    "### ImportError for bitsandbytes\n",
    "Run: `!pip install bitsandbytes`\n",
    "\n",
    "Note: On Windows, bitsandbytes may require additional setup.\n",
    "\n",
    "## Performance Notes\n",
    "\n",
    "| Configuration | VRAM Usage | Speed (tokens/sec) |\n",
    "|--------------|------------|-------------------|\n",
    "| FP16         | ~14 GB     | ~30-50 (GPU)      |\n",
    "| 8-bit        | ~4 GB      | ~20-35 (GPU)      |\n",
    "| 4-bit        | ~2-3 GB    | ~15-25 (GPU)      |\n",
    "| CPU          | 0 GB       | ~2-5 (CPU)        |\n",
    "\n",
    "*Speed varies based on hardware and prompt complexity*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
